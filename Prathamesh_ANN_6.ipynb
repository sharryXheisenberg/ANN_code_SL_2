{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiclass:\n",
    "    def __init__(self,input_size,hidden_size,output_size,learning_rate):\n",
    "        self.input_size=input_size\n",
    "        self.output_size=output_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.learning_rate=learning_rate\n",
    "        #Initialize weights and bias\n",
    "        self.Vij=np.random.uniform(-0.5,0.5,(self.input_size,self.hidden_size))\n",
    "        self.Wjk=np.random.uniform(-0.5,0.5,(self.hidden_size,self.output_size))\n",
    "        self.Vbj=np.random.uniform(-0.5,0.5,(1,self.hidden_size))\n",
    "        self.Wbk=np.random.uniform(-0.5,0.5,(1,self.output_size))\n",
    "\n",
    "    \n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def forward_propogation(self,x):\n",
    "            self.hidden_input=np.dot(x,self.Vij)+self.Vbj\n",
    "            self.hidden_output=self.relu(self.hidden_input)\n",
    "            self.output_input=np.dot(self.hidden_output,self.Wjk) + self.Wbk\n",
    "            self.output_final=self.relu(self.output_input)\n",
    "            return self.output_final\n",
    "    def backword_propogation(self,x,y,output):\n",
    "        error=y-output\n",
    "        #Weight correction in weights\n",
    "        delta_K=error * self.relu_derivative(output)#Output layer error\n",
    "        delta_in=delta_K.dot(self.Wjk.T)#Hidden Unit error\n",
    "        delta_J=delta_in * self.relu_derivative(self.hidden_output)#Hidden Layer error\n",
    "\n",
    "\n",
    "        # Weigth adjustent in  Weight vector between Output and hidden layer\n",
    "        self.Wjk+=self.hidden_output.T.dot(delta_K) * self.learning_rate\n",
    "\n",
    "        #Change in bias between output and hidden layer\n",
    "        self.Wbk+=np.sum(delta_K)*self.learning_rate\n",
    "        \n",
    "        # Weigth adjustent in  Weight vector between input and hidden layer\n",
    "        self.Vij+=x.T.dot(delta_J) * self.learning_rate\n",
    "\n",
    "        #Change in bias between input and hidden layer \n",
    "        self.Vbj+=np.sum(delta_J)*self.learning_rate\n",
    "    def train_model(self,x,y,epochs):\n",
    "        for epoch in range(epochs):\n",
    "            output=self.forward_propogation((x))\n",
    "            self.backword_propogation(x,y,output)\n",
    "            if epoch % 1000 ==0 :\n",
    "                print(f'Epoch{epoch}:Error{np.mean(np.abs(y-output))}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_iris()\n",
    "x = data.data\n",
    "y = data.target.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train_encoded = encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = x_train.shape[1]\n",
    "hidden_size= 100  # Set the number of neurons in hidden layer\n",
    "output_size = len(np.unique(y))  # More than 1 neuron in the output layer\n",
    "learning_rate = 0.1  # Adjust the learning rate as needed\n",
    "epochs = 10000  # Adjust the number of epochs as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0:Error1.3305666355811094\n",
      "Epoch1000:Error0.3333333333333333\n",
      "Epoch2000:Error0.3333333333333333\n",
      "Epoch3000:Error0.3333333333333333\n",
      "Epoch4000:Error0.3333333333333333\n",
      "Epoch5000:Error0.3333333333333333\n",
      "Epoch6000:Error0.3333333333333333\n",
      "Epoch7000:Error0.3333333333333333\n",
      "Epoch8000:Error0.3333333333333333\n",
      "Epoch9000:Error0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ann = Multiclass(input_size, hidden_size, output_size, learning_rate)\n",
    "ann.train_model(x_train, y_train_encoded, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.33333333333333 %\n"
     ]
    }
   ],
   "source": [
    "output = ann.forward_propogation(x_test)\n",
    "predicted_labels = np.argmax(output, axis=1)\n",
    "accuracy = np.mean(predicted_labels == y_test.flatten())\n",
    "print(\"Accuracy:\", accuracy*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
